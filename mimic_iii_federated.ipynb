{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING - In Progress\n",
    "\n",
    "import psycopg2\n",
    "from datetime import timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import scipy.io\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, GRU, LSTM, Conv2D, MaxPooling2D, Activation\n",
    "\n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import collections\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#%load_ext tensorboard\n",
    "\n",
    "# CHANGE SQL FILE to flicu_pivoted_lab to only include ICU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKBACK = [5, 10, 24] # lookback (5h, 10h, 24h)\n",
    "LOOKAHEAD = [3, 6, 12] # lookahead (3h, 6h, 12h)\n",
    "MODEL = ['GRU', 'LSTM'] # model type ('GRU', 'LSTM')\n",
    "CLIENTS = [2, 4, 8] # number of federated learning clients (2, 4, 8)\n",
    "bs = 256 # batch size\n",
    "lr = 0.01 # learning rate\n",
    "comms_round = 50 # global epochs\n",
    "model_loc='federated_models/' # folder for models to be saved in \n",
    "window_loc='windows_trial/' # folder to get windows files from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")#!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "#-----\n",
    "columns_pred = ['sysbp','diasbp','heartrate','tempc','resprate','wbc','ph','spo2','admission_age'] # parameters to use for the prediction task\n",
    "columns_pred_idx = [x for x in range(len(columns_pred))]\n",
    "#-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_load_process(file_name, lb):\n",
    "\n",
    "    # load windows\n",
    "    windows = pd.read_pickle(file_name)\n",
    "\n",
    "    # sort dataframe by labels\n",
    "    windows.sort_values(by=['label'])\n",
    "\n",
    "    # split into X and y\n",
    "    x_set_prime = np.array(windows['window'])\n",
    "    x_set = np.zeros((len(x_set_prime),lb, len(columns_pred_idx)), float)\n",
    "    for i in range(len(x_set_prime)):\n",
    "        x_set[i] = x_set_prime[i]\n",
    "\n",
    "    y_set = np.array(windows['label'])\n",
    "\n",
    "    #print(x_set.shape)\n",
    "    #print(y_set.shape)\n",
    "    return x_set, y_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the normalization of the values based on the training data\n",
    "# input : (data for standardisation parameter extraction, data to apply standardisation on)\n",
    "def normalize(x_par,x_out):\n",
    "    # one mean and std dev value for every parameter\n",
    "    x_copy = x_par.copy()\n",
    "    x_copy = x_copy.reshape(-1,len(columns_pred_idx)).transpose()\n",
    "    \n",
    "    means = np.zeros((len(columns_pred_idx)))\n",
    "    stds = np.zeros((len(columns_pred_idx)))\n",
    "\n",
    "    for i in range(0,len(x_copy)):\n",
    "        means[i] = np.mean(x_copy[i])\n",
    "        stds[i] = np.std(x_copy[i])\n",
    "\n",
    "    # normalize the values\n",
    "    x_out[:,:,:] -= means\n",
    "    x_out[:,:,:] /= stds\n",
    "    \n",
    "    #transform from float64 to float32 format\n",
    "    x_out = x_out.astype('float32')\n",
    "    \n",
    "    return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equalize the overall class proportions by duplicating the sepsis cases in the training set\n",
    "def expand(x_train,y_train):\n",
    "    \"\"\"\n",
    "    Info:\n",
    "    =====\n",
    "        - the function multiplies the sepsis cases in the training to improve the ANN training\n",
    "    \n",
    "    Parameter description:\n",
    "    ======================\n",
    "    \n",
    "    x_train\n",
    "    -------\n",
    "        - the extracted features from the training set\n",
    "        - x_train.shape[0] -> the samples\n",
    "        - x_train.shape[1] -> the parameter values for each timestamp of the sequence (in case of sequence learning (RNN))\n",
    "                           -> the higher level extracted features (in case of learning with extracted features)\n",
    "        - x_train.shape[2] -> the parameters (in case of sequence learning)\n",
    "                           -> not existent (in case of learning with extracted features)\n",
    "    \n",
    "    y_train\n",
    "    -------\n",
    "        - the labels of the corresponding samples\n",
    "        - vector with the same length of x_train[0]\n",
    "    \"\"\"\n",
    "    n = np.int((y_train.shape[0]-y_train.sum())/y_train.sum()) # duplicate n times\n",
    "    if len(x_train.shape) == 3:\n",
    "        sh = (np.int(n*y_train.sum()),x_train.shape[1],x_train.shape[2])\n",
    "    else:\n",
    "        sh = (np.int(n*y_train.sum()),x_train.shape[1])\n",
    "    x_train_apdx = np.zeros(sh)\n",
    "    x_train_sepsis = x_train[y_train>0]\n",
    "    starti = 0\n",
    "    endi = np.int(y_train.sum())\n",
    "    for i in range(0,n):\n",
    "        x_train_apdx[starti:endi] = x_train_sepsis\n",
    "        starti = endi\n",
    "        endi = np.int((i+2) * y_train.sum())\n",
    "    x_train = np.concatenate((x_train,x_train_apdx),axis=0)\n",
    "    y_train = np.concatenate((y_train,np.ones(x_train_apdx.shape[0])))\n",
    "\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sens(y_true, y_pred):\n",
    "    \n",
    "    th = tf.constant(0.5, dtype=tf.float32)\n",
    "    sv = tf.constant(1e-6, dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, dtype=tf.bool)\n",
    "    y_pred = y_pred > th # as the network outputs a value between [0,1] according to the sigmoid activation of the last unit\n",
    "    \n",
    "    y_true_ones = tf.boolean_mask(y_true, y_true)\n",
    "    relating_y_pred = tf.boolean_mask(y_pred, y_true)\n",
    "\n",
    "    res = tf.equal(y_true_ones, relating_y_pred)\n",
    "    res = tf.cast(res, dtype=tf.float32)\n",
    "    cM_11 = tf.reduce_sum(res) + sv # true positives cases (1e6 added to avoid division by 0 and therefor nan as result)\n",
    "    cM_10 = tf.cast(tf.size(res), dtype=tf.float32) - cM_11 + sv # false negative cases (1e6 added to avoid division by 0 and therefor nan as result)\n",
    "    sens = cM_11/(cM_11+cM_10)\n",
    "\n",
    "    return sens\n",
    "\n",
    "\n",
    "def spec(y_true, y_pred):\n",
    "    \n",
    "    th = tf.constant(0.5, dtype=tf.float32)\n",
    "    sv = tf.constant(1e-6, dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, dtype=tf.bool)\n",
    "    y_pred = y_pred > th # as the network outputs a value between [0,1] according to the sigmoid activation of the last unit\n",
    "\n",
    "    y_true_zeros = tf.boolean_mask(y_true, tf.logical_not(y_true))\n",
    "    relating_y_pred = tf.boolean_mask(y_pred, tf.logical_not(y_true))\n",
    "    y_true_zeros = tf.logical_not(y_true_zeros)\n",
    "    relating_y_pred = tf.logical_not(relating_y_pred)\n",
    "\n",
    "    res = tf.equal(y_true_zeros, relating_y_pred)\n",
    "    res = tf.cast(res, dtype=tf.float32)\n",
    "    cM_00 = tf.reduce_sum(res) + sv # true negative cases (1e6 added to avoid division by 0 and therefor nan as result)\n",
    "    cM_01 = tf.cast(tf.size(res), dtype=tf.float32) - cM_00 + sv # false positive cases (1e6 added to avoid division by 0 and therefor nan as result)\n",
    "    spec = cM_00/(cM_00+cM_01)\n",
    "    \n",
    "    return spec\n",
    "\n",
    "def aurocSKL(y_true, y_pred, digits=3):\n",
    "    \n",
    "    res = roc_auc_score(y_true, y_pred) # function from scikitlearn (skl)\n",
    "    res = round(res, digits)\n",
    "\n",
    "    return res\n",
    "\n",
    "def auprcSKL(y_true, y_pred, digits=3):\n",
    "    \n",
    "    res = average_precision_score(y_true, y_pred) # function from scikitlearn (skl)\n",
    "    res = round(res, digits)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_MLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(GRU(40,recurrent_dropout=0.2,input_shape=(shape, classes),return_sequences=True))\n",
    "        model.add(GRU(40,recurrent_dropout=0.2,input_shape=(shape, classes)))\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        return model\n",
    "\n",
    "class LSTM_MLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(40,recurrent_dropout=0.2,input_shape=(shape, classes),return_sequences=True))\n",
    "        model.add(LSTM(40,recurrent_dropout=0.2,input_shape=(shape, classes)))\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test, model, fold, verbose=1):\n",
    "    Y_test = tf.expand_dims(Y_test, 1)\n",
    "    cce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(Y_test,(logits > 0.5).astype(np.float32))\n",
    "    auroc = aurocSKL(Y_test,logits)\n",
    "    auprc = auprcSKL(Y_test,logits)\n",
    "    f1 = f1_score(Y_test,(logits > 0.5).astype(np.float32))\n",
    "    if verbose == 1:\n",
    "        print('FOLD: {} | global_acc: {:.3%} | global_loss: {:.3%} | global_auroc: {:.3%} | global_auprc: {:.3%} | global_f1: {:.3%}'.format(fold, acc, loss, auroc, auprc, f1))\n",
    "    return acc, loss, auroc, auprc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Members (clients) as Data Shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(train_list, label_list, bs=32, num_clients=3, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(train_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    shards_train = []\n",
    "    shards_val = []\n",
    "    # normalize and expand shards\n",
    "    for shard in shards:\n",
    "        # unzip train data and labels\n",
    "        x_train_c = np.array([i.tolist() for i, j in shard])\n",
    "        y_train_c = np.array([j.tolist() for i, j in shard])\n",
    "\n",
    "        # split shard into training and validation sets\n",
    "        x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(x_train_c,y_train_c,test_size=0.25,stratify=y_train_c,random_state=1)\n",
    "        \n",
    "        # standardise training and validation data\n",
    "        x_train_c = normalize(x_train_c,x_train_c)\n",
    "        x_val_c = normalize(x_train_c,x_val_c)\n",
    "\n",
    "        # expand training data\n",
    "        x_train_c, y_train_c = expand(x_train_c,y_train_c)\n",
    "\n",
    "        # process and batch the training data for each client\n",
    "        shards_train.append(tf.data.Dataset.from_tensor_slices((x_train_c, y_train_c)).shuffle(len(y_train_c)).batch(bs))\n",
    "        shards_val.append(tf.data.Dataset.from_tensor_slices((x_val_c, y_val_c)).batch(len(y_val_c)))\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards_train) == len(shards_val) == len(client_names))\n",
    "\n",
    "    clients_train = {client_names[i] : shards_train[i] for i in range(len(client_names))}\n",
    "    clients_val = {client_names[i] : shards_val[i] for i in range(len(client_names))}\n",
    "\n",
    "    return clients_train, clients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss='binary_crossentropy'\n",
    "metrics = ['accuracy', sens, spec]\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 4-fold stratified cross-validation\n",
    "2. split in the ratio of 9:3:4\n",
    "3. batch size of 32\n",
    "4. look-back window lengths (5h, 10h, 24h) ~\n",
    "5. prediction times (3h, 6h, 12h) ~\n",
    "6. FL clients are 2, 4, and 8 ~\n",
    "7. learning rate of 0.01\n",
    "8. 50 FL rounds\n",
    "9. local model 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x_set, y_set, lb, pt, model_type, client_cnt, nsplits, comms_round=30, verbose_round=0, verbose_fold=0):\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_set = label_encoder.fit_transform(y_set)\n",
    "\n",
    "    # generate stratified folds for crossvalidation\n",
    "    skf = StratifiedKFold(n_splits=nsplits,shuffle=True,random_state=1) # specify random_state to produce the same random numbers each time\n",
    "\n",
    "    # initialize stratified kFold generator\n",
    "    splits = skf.split(x_set,y_set)\n",
    "\n",
    "    split_c = -1\n",
    "\n",
    "    fold_cnt = 0\n",
    "\n",
    "    model_history = []\n",
    "    for train_idx, test_idx in splits:\n",
    "        fold_cnt += 1\n",
    "        split_c += 1\n",
    "        x_train = x_set[train_idx]\n",
    "        y_train = y_set[train_idx]\n",
    "        x_test = x_set[test_idx]\n",
    "        y_test = y_set[test_idx]\n",
    "\n",
    "        # normalize testing data based on global training set\n",
    "        x_test = normalize(x_train,x_test)\n",
    "\n",
    "        #print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "        #create, process and batch clients\n",
    "        clients_train, clients_val = create_clients(x_train, y_train, bs, num_clients=client_cnt, initial='client')\n",
    "\n",
    "        #process and batch the test set  \n",
    "        test_batched = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(len(y_test))\n",
    "        \n",
    "        \n",
    "        #initialize global model\n",
    "        smlp_global = GRU_MLP()\n",
    "        if model_type == 'LSTM':\n",
    "            smlp_global = LSTM_MLP()\n",
    "        global_model = smlp_global.build(x_train.shape[1], x_train.shape[2])\n",
    "                \n",
    "        #commence global training loop\n",
    "        global_val_metrics = []\n",
    "        for comm_round in range(comms_round):\n",
    "                    \n",
    "            # get the global model's weights - will serve as the initial weights for all local models\n",
    "            global_weights = global_model.get_weights()\n",
    "            \n",
    "            #initial list to collect local model weights after scalling\n",
    "            scaled_local_weight_list = list()\n",
    "\n",
    "            #randomize client data - using keys\n",
    "            client_names= list(clients_train.keys())\n",
    "            random.shuffle(client_names)\n",
    "            \n",
    "            #loop through each client and create new local model\n",
    "            local_metrics = []\n",
    "            for client in client_names:\n",
    "                smlp_local = GRU_MLP()\n",
    "                if model_type == 'LSTM':\n",
    "                    smlp_local = LSTM_MLP()\n",
    "                local_model = smlp_local.build(x_train.shape[1], x_train.shape[2])\n",
    "                local_model.compile(loss=loss, \n",
    "                            optimizer=optimizer, \n",
    "                            metrics=metrics)\n",
    "                \n",
    "                #set local model weight to the weight of the global model\n",
    "                local_model.set_weights(global_weights)\n",
    "                \n",
    "                #fit local model with client's data\n",
    "                local_model.fit(clients_train[client], epochs=3, verbose=0)\n",
    "                \n",
    "                #scale the model weights and add to list\n",
    "                scaling_factor = weight_scalling_factor(clients_train, client)\n",
    "                scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "                scaled_local_weight_list.append(scaled_weights)\n",
    "                #test local model\n",
    "                for(X_val, Y_val) in clients_val[client]:\n",
    "                    local_acc, local_loss, local_auroc, local_auprc, local_f1 = test_model(X_val, Y_val, local_model, comm_round, verbose=0)\n",
    "                    local_metrics.append([local_acc, local_loss, local_auroc, local_auprc, local_f1])\n",
    "                \n",
    "            #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "            average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "            \n",
    "            #update global model \n",
    "            global_model.set_weights(average_weights)\n",
    "\n",
    "            # transpose local metrics and average\n",
    "            local_metrics = np.array(local_metrics).T.tolist()\n",
    "            local_metrics = [sum(r)/len(r) for r in local_metrics]\n",
    "\n",
    "            if verbose_round == 1:\n",
    "                print('round: {} | global_val_acc: {:.3%} | global_val_loss: {:.3%} | global_val_auroc: {:.3%} | global_val_auprc: {:.3%} | global_val_f1_score: {:.3%}'.format(comm_round, local_metrics[0], local_metrics[1], local_metrics[2], local_metrics[3], local_metrics[4]))\n",
    "            \n",
    "            #clear session to free memory after each communication round\n",
    "            K.clear_session()\n",
    "                \n",
    "            # stop criteria (on AUROC)\n",
    "            global_val_metrics.append(local_metrics)\n",
    "            if len(global_val_metrics) >= 21:\n",
    "                if global_val_metrics[comm_round-20][1] > global_val_metrics[-1][1]: # check for stagnation/overfitting\n",
    "                    print('Stopped early: round ', comm_round)\n",
    "                    break\n",
    "\n",
    "        #test global model and print out metrics for current fold\n",
    "        global_metrics = []\n",
    "        for(X_test, Y_test) in test_batched:\n",
    "            global_acc, global_loss, global_auroc, global_auprc, global_f1 = test_model(X_test, Y_test, global_model, fold_cnt, verbose=verbose_fold)\n",
    "            global_metrics = [global_acc, global_loss, global_auroc, global_auprc, global_f1]\n",
    "\n",
    "        model_history.append([global_metrics[0], global_metrics[1], global_metrics[2], global_metrics[3], global_metrics[4], global_model])\n",
    "\n",
    "    # identify best model (auroc) across folds\n",
    "    # format: model, lookback, lookahead, clients, accuracy, loss, auroc, auprc\n",
    "    best_idx = np.array(model_history).T.tolist()[2].index(max(np.array(model_history).T.tolist()[2]))\n",
    "    best = model_history[best_idx]\n",
    "    \n",
    "    # save model\n",
    "    fname = str(model_type)+'_Model_lookback_'+str(lb)+'_lookahead_'+str(pt)+'_flClients_'+str(client_cnt)+'_aurocTest_'+str(best[2])+'_auprcTest_'+str(best[3])+'_f1score_'+str(best[4])+'.h5'\n",
    "    best[4].save(model_loc+fname)   \n",
    "\n",
    "    cross_val_best.append([str(model_type), lb, pt, client_cnt, best[0], best[1], best[2], best[3], best[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STARTING--- model:GRU lookback:5 lookahead:3 clients:2 -----\n"
     ]
    }
   ],
   "source": [
    "#-------------\n",
    "nsplits = 2\n",
    "verbose_round = 0\n",
    "verbose_fold = 1\n",
    "#-------------\n",
    "\n",
    "cross_val_best = [['model', 'lookback', 'lookahead', 'clients', 'accuracy', 'loss', 'auroc', 'auprc', 'f1_score']]\n",
    "\n",
    "for lb in LOOKBACK:\n",
    "    for pt in LOOKAHEAD:\n",
    "\n",
    "        # load and process windows\n",
    "        windows_file = window_loc+'Windows_lookback_'+str(lb)+'_lookahead_'+str(pt)+'.pkl'\n",
    "        x_set, y_set = windows_load_process(windows_file, lb)\n",
    "\n",
    "        for client_cnt in CLIENTS:\n",
    "\n",
    "            for model_type in MODEL:\n",
    "\n",
    "                print('---STARTING--- model:'+model_type+' lookback:'+str(lb)+' lookahead:'+str(pt)+' clients:'+str(client_cnt)+' -----')\n",
    "                # train and test\n",
    "                best_model_par = process(x_set, y_set, lb, pt, model_type, client_cnt, nsplits, comms_round, verbose_round, verbose_fold)\n",
    "                cross_val_best.append(best_model_par)\n",
    "                print('---------------------------------------------------------------------------')\n",
    "\n",
    "print(cross_val_best)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bbe4b04bbda9182dee05bae70819db83e74172125e017501bfa1cf45b554296"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
